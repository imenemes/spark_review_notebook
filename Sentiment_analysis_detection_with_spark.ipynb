{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "XpIPVWX_tfwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dUquyFFtjHR",
        "outputId": "df89e860-195f-42fb-a624-b688f959bc80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-25 22:11:05--  https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.95.219, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400395283 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.0-bin-hadoop3.tgz.1’\n",
            "\n",
            "spark-3.5.0-bin-had 100%[===================>] 381.85M  18.7MB/s    in 22s     \n",
            "\n",
            "2023-12-25 22:11:27 (17.6 MB/s) - ‘spark-3.5.0-bin-hadoop3.tgz.1’ saved [400395283/400395283]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf spark-3.5.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "9OPLIXVntlxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ],
      "metadata": {
        "id": "U-Oks8qctvz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "id": "DpX93ZpItyxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72af7c6b-17ef-4e4f-e3f2-058f3e5b25f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "tUTQfaC7t1YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Authenticate and mount the drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXXXrsu5eVB2",
        "outputId": "c3835733-f8f2-479f-eea5-1084a96d5ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.sql.functions import col, isnan, when, count\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml import Pipeline\n",
        "from mpl_toolkits.mplot3d import Axes3D\n"
      ],
      "metadata": {
        "id": "_HVbJUsQtR5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initiation de la session Spark\n",
        "spark=SparkSession.builder.appName(\"ProjetFinal\").getOrCreate()"
      ],
      "metadata": {
        "id": "2MAxbNlMtVWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chargement des données\n",
        "data = spark.read.csv(\"/content/drive/MyDrive/Data.csv\",header=True, inferSchema=True)\n",
        "#spark.read.option(\"multiLine\", True).csv(\"/content/drive/MyDrive/mental_health.csv\",header=True, inferSchema=True)\n",
        "# enlever les colonnes vide\n",
        "data = data.na.drop()\n",
        "data.show(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNeS7XjLuD4Z",
        "outputId": "3b977c79-9629-4688-c26f-8dc64f3e2e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|              Review|Rating|\n",
            "+--------------------+------+\n",
            "|nice hotel expens...|     4|\n",
            "|ok nothing specia...|     2|\n",
            "|nice rooms not 4*...|     3|\n",
            "|unique, great sta...|     5|\n",
            "|great stay great ...|     5|\n",
            "|love monaco staff...|     5|\n",
            "|cozy stay rainy c...|     5|\n",
            "|excellent staff, ...|     4|\n",
            "|hotel stayed hote...|     5|\n",
            "|excellent stayed ...|     5|\n",
            "|poor value stayed...|     2|\n",
            "|nice value seattl...|     4|\n",
            "|nice hotel good l...|     4|\n",
            "|nice hotel not ni...|     3|\n",
            "|great hotel night...|     4|\n",
            "|horrible customer...|     1|\n",
            "|disappointed say ...|     2|\n",
            "|fantastic stay mo...|     5|\n",
            "|good choice hotel...|     5|\n",
            "|hmmmmm say really...|     3|\n",
            "|service service s...|     5|\n",
            "|excellent stay, d...|     5|\n",
            "|good value downto...|     4|\n",
            "|hotel monaco grea...|     5|\n",
            "|great location ne...|     2|\n",
            "|n't mind noise pl...|     3|\n",
            "|loved, stayed war...|     4|\n",
            "|met expectations ...|     3|\n",
            "|nice hotel husban...|     4|\n",
            "|good hotel not la...|     4|\n",
            "|good choice seatt...|     4|\n",
            "|great location ex...|     4|\n",
            "|noise airconditio...|     1|\n",
            "|good location poo...|     2|\n",
            "|good place spendi...|     4|\n",
            "|nice hotel trip s...|     4|\n",
            "|great value seatt...|     4|\n",
            "|gem hotel absolut...|     5|\n",
            "|pretty good value...|     4|\n",
            "|average nice stay...|     4|\n",
            "|bad choice, booke...|     1|\n",
            "|good value hotel ...|     4|\n",
            "|warwick bad good ...|     2|\n",
            "|great service war...|     4|\n",
            "|austin powers dec...|     2|\n",
            "|great location n'...|     2|\n",
            "|pay read reviews ...|     3|\n",
            "|not bad location ...|     3|\n",
            "|remarkable hotel ...|     5|\n",
            "|great location fr...|     5|\n",
            "|excellent way sta...|     5|\n",
            "|amazing location ...|     4|\n",
            "|outstanding choic...|     5|\n",
            "|great hotel great...|     5|\n",
            "|expensive, not bi...|     3|\n",
            "|nice place, lunat...|     5|\n",
            "|okay not amazing ...|     3|\n",
            "|boutique charmer ...|     5|\n",
            "|loved inn inn mar...|     5|\n",
            "|n't asked better ...|     5|\n",
            "|nice needs undati...|     4|\n",
            "|wonderful locatio...|     5|\n",
            "|tempur-pedic beds...|     5|\n",
            "|loved inn stayed ...|     5|\n",
            "|worth money locat...|     4|\n",
            "|hated inn terribl...|     1|\n",
            "|loved inn husband...|     5|\n",
            "|ace not place hus...|     3|\n",
            "|pay location loca...|     3|\n",
            "|ace grunge lives ...|     1|\n",
            "|ace hotel, reason...|     4|\n",
            "|disappointed arra...|     2|\n",
            "|ace hotel awesome...|     5|\n",
            "|perfect way, stay...|     5|\n",
            "|interesting comfo...|     4|\n",
            "|great hotel non-t...|     5|\n",
            "|stay clear, inter...|     1|\n",
            "|single rooms like...|     1|\n",
            "|nice hotel husban...|     4|\n",
            "|seattle crown pla...|     2|\n",
            "|priority award fl...|     2|\n",
            "|great hotel hotel...|     5|\n",
            "|great stay elevat...|     5|\n",
            "|holiday inn, 1st ...|     2|\n",
            "|stay liked good r...|     4|\n",
            "|good location val...|     4|\n",
            "|good experience h...|     5|\n",
            "|pleasant experien...|     4|\n",
            "|worst hotel exper...|     1|\n",
            "|close convention ...|     4|\n",
            "|not bad clean com...|     4|\n",
            "|loved inn queen a...|     5|\n",
            "|quaint not rundow...|     1|\n",
            "|comfortable apt.-...|     4|\n",
            "|not bad place sta...|     4|\n",
            "|excellent stay st...|     4|\n",
            "|return going seat...|     4|\n",
            "|terrible hotel ap...|     1|\n",
            "|great price okay ...|     3|\n",
            "|old dumpy place p...|     1|\n",
            "+--------------------+------+\n",
            "only showing top 100 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous constatons que les rating sont de 1 à 5 , nous voulons résoudre un problème de classification binaire sentiment positif ou négatif , nous retraitons donc la données ."
      ],
      "metadata": {
        "id": "RQ1joY1Oxx9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ratings(rating):\n",
        "    if rating>3 and rating<=5:\n",
        "        return 1\n",
        "    if rating>0 and rating<=3:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "xQjl5ot4xxFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import udf\n",
        "#appliquer la fonction rating pour avoir deux classes\n",
        "\n",
        "data = data.withColumn(\"Sentiment\", udf(ratings, IntegerType())(data[\"Rating\"]))"
      ],
      "metadata": {
        "id": "vP62ejQdyCUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQI998ekf6p2",
        "outputId": "7c6781d0-d0f3-440a-c082-a0e30f713eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+---------+\n",
            "|              Review|Rating|Sentiment|\n",
            "+--------------------+------+---------+\n",
            "|nice hotel expens...|     4|        1|\n",
            "|ok nothing specia...|     2|        0|\n",
            "|nice rooms not 4*...|     3|        0|\n",
            "|unique, great sta...|     5|        1|\n",
            "|great stay great ...|     5|        1|\n",
            "|love monaco staff...|     5|        1|\n",
            "|cozy stay rainy c...|     5|        1|\n",
            "|excellent staff, ...|     4|        1|\n",
            "|hotel stayed hote...|     5|        1|\n",
            "|excellent stayed ...|     5|        1|\n",
            "|poor value stayed...|     2|        0|\n",
            "|nice value seattl...|     4|        1|\n",
            "|nice hotel good l...|     4|        1|\n",
            "|nice hotel not ni...|     3|        0|\n",
            "|great hotel night...|     4|        1|\n",
            "|horrible customer...|     1|        0|\n",
            "|disappointed say ...|     2|        0|\n",
            "|fantastic stay mo...|     5|        1|\n",
            "|good choice hotel...|     5|        1|\n",
            "|hmmmmm say really...|     3|        0|\n",
            "+--------------------+------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtzMB8X9eKC4",
        "outputId": "6756a3cb-5728-4efe-a426-acdcd8accbba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Review: string (nullable = true)\n",
            " |-- Rating: integer (nullable = true)\n",
            " |-- Sentiment: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- Préparation des données**"
      ],
      "metadata": {
        "id": "O1L6IjG_jwrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enlever les espaces"
      ],
      "metadata": {
        "id": "nnTzBIQIW4F3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijgEmfooeKC5",
        "outputId": "ebc5154e-6c4a-4fae-854d-95380ba6ac9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Review='nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous reviews did valet parking, check quick easy, little disappointed non-existent view room room clean nice size, bed comfortable woke stiff neck high pillows, not soundproof like heard music room night morning loud bangs doors opening closing hear people talking hallway, maybe just noisy neighbors, aveda bath products nice, did not goldfish stay nice touch taken advantage staying longer, location great walking distance shopping, overall nice experience having pay 40 parking night,', Rating=4, Sentiment=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from pyspark.sql.functions import regexp_replace, trim\n",
        "# nettoyage de la colonne 'Review' d'un DataFrame en supprimant les espaces redondants\n",
        "data = data.withColumn('Review', regexp_replace('Review', '[\\s]{2,}', ''))\n",
        "# élimine les espaces inutiles au début ou à la fin de chaque valeur de cette colonne\n",
        "data = data.withColumn('Review', trim(data.Review))\n",
        "data.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TquYUcLkeKC5"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, lower\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gYkGEcFNeKC5"
      },
      "outputs": [],
      "source": [
        "# Mettre tout en minuscule\n",
        "data = data.withColumn(\"Review\", lower(col('Review')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enlever les lignes vides après nettoyage\n",
        "data = data.filter(data.Review != '')"
      ],
      "metadata": {
        "id": "07kBTzvKXkLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "asULoukPeKC5"
      },
      "outputs": [],
      "source": [
        "# initialiser le tokeniser\n",
        "tokenizer = Tokenizer(inputCol=\"Review\", outputCol=\"words\")\n",
        "# transformer la colonne text en mots (token)\n",
        "wordsData = tokenizer.transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-gqnLmkNeKC6"
      },
      "outputs": [],
      "source": [
        "# Création du transformateur StopWordsRemover\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "# l'appliquer sur nos données pour enlever les stopwords\n",
        "Nstopwords = remover.transform(wordsData)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2- Vectorisation**"
      ],
      "metadata": {
        "id": "JxXH4IYodLcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a- Word2vec**"
      ],
      "metadata": {
        "id": "PQTvno-KdR3O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jXPQLHbceKC6"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Word2Vec\n",
        "# création du modèle w2vec\n",
        "word2Vec = Word2Vec(vectorSize=150, inputCol=\"filtered\", outputCol=\"features\")\n",
        "# entrainement du modèle word2vec\n",
        "word2vec_data = word2Vec.fit(Nstopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "U2tg6FVVeKC6"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Word2VecModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sauvegarder les vecteurs\n",
        "word2vec_data.save(\"/content/drive/MyDrive/w2vec\")"
      ],
      "metadata": {
        "id": "OKNnnvxRNzlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le modèle Word2Vec sauvegardé\n",
        "word2vec_data = Word2VecModel.load(\"/content/drive/MyDrive/w2vec\")"
      ],
      "metadata": {
        "id": "WM9ibjmrJaw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_cJ8JqK_eKC7"
      },
      "outputs": [],
      "source": [
        "#appliquer le modèle entrainé à nos datas\n",
        "w2v_data = word2vec_data.transform(Nstopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Division des données en données de test et de train**"
      ],
      "metadata": {
        "id": "7Th-9yIfhQOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La méthode randomSplit() de Spark MLlib divise le DataFrame en ensembles d'apprentissage et de test en utilisant une répartition aléatoire selon les proportions spécifiées. Cependant, elle ne garantit pas une répartition égale des classes dans chaque ensemble.\n",
        "\n",
        "Si les classes ne sont pas équilibrées dans le DataFrame original, la répartition des classes peut différer dans les ensembles d'apprentissage et de test générés. Par conséquent, il est possible que la proportion de chaque classe dans les ensembles d'apprentissage et de test ne soit pas égale.\n",
        "\n",
        "Pour obtenir des ensembles d'apprentissage et de test avec une répartition égale des classes, vous pouvez envisager d'utiliser des méthodes comme stratified sampling, qui garantissent une répartition équilibrée des classes dans les ensembles. Malheureusement, Spark MLlib ne propose pas directement de méthode stratifiée pour la division des ensembles via randomSplit().\n",
        "\n",
        "Une alternative serait d'effectuer une division manuelle en filtrant les données par classe et en utilisant randomSplit() séparément pour chaque classe, tout en maintenant la proportion souhaitée pour chaque classe dans les ensembles d'apprentissage et de test. Cela garantira une répartition plus équilibrée des classes dans les ensembles."
      ],
      "metadata": {
        "id": "wgFg0HFNhEjs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RD_Lhn9HeKC7"
      },
      "outputs": [],
      "source": [
        "# split dataframes between 0s and 1s\n",
        "zeros = w2v_data.filter(w2v_data[\"Sentiment\"]==0)\n",
        "ones = w2v_data.filter(w2v_data[\"Sentiment\"]==1)\n",
        "# split datasets into training and testing\n",
        "train0, test0 = zeros.randomSplit([0.8,0.2])\n",
        "train1, test1 = ones.randomSplit([0.8,0.2])\n",
        "# merge datasets back together\n",
        "train = train0.union(train1)\n",
        "test = test0.union(test1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Z48uz7gbeKC7"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utilisation de la méthode des arbres de décision**"
      ],
      "metadata": {
        "id": "mqi15NRmharm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici on utilise PySpark pour définir un classificateur d'arbres de décision à l'aide de la classe DecisionTreeClassifier du module pyspark.ml.classification. Ensuite, on crée une grille de paramètres (paramGrid) à explorer pour l'optimisation des hyperparamètres du classificateur.\n",
        "\n",
        "\n",
        "Cette configuration permet d'explorer différentes valeurs pour les paramètres maxDepth et maxBins de l'arbre de décision afin d'optimiser ses performances pour un problème d'apprentissage supervisé."
      ],
      "metadata": {
        "id": "gK5Dgey00FQA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oBsu0CJ3eKC7"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "dtparamGrid = (ParamGridBuilder()\n",
        "             .addGrid(dt.maxDepth, [2, 5, 10, 20])\n",
        "             .addGrid(dt.maxBins, [10, 20, 40])\n",
        "             .build())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "train = train.withColumnRenamed(\"Sentiment\", \"label\")\n",
        "test = test.withColumnRenamed(\"Sentiment\", \"label\")\n",
        "test.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZFD7cgv0cE6",
        "outputId": "257c24db-99e4-420d-c16b-4ca094f0087b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+-----+--------------------+--------------------+--------------------+\n",
            "|              Review|Rating|label|               words|            filtered|            features|\n",
            "+--------------------+------+-----+--------------------+--------------------+--------------------+\n",
            "|2-star motel hote...|     1|    0|[2-star, motel, h...|[2-star, motel, h...|[0.00359839154407...|\n",
            "|2.5 stars masqera...|     2|    0|[2.5, stars, masq...|[2.5, stars, masq...|[0.01139937909239...|\n",
            "|3 king size bed, ...|     2|    0|[3, king, size, b...|[3, king, size, b...|[-0.0056014036401...|\n",
            "|3 star lobby 2 st...|     2|    0|[3, star, lobby, ...|[3, star, lobby, ...|[0.04633747633532...|\n",
            "|5 day away just d...|     2|    0|[5, day, away, ju...|[5, day, away, da...|[0.02343787890858...|\n",
            "|50/50 response go...|     2|    0|[50/50, response,...|[50/50, response,...|[-0.0143492023811...|\n",
            "|absolutely awful ...|     1|    0|[absolutely, awfu...|[absolutely, awfu...|[-0.0090345171112...|\n",
            "|abysmal service e...|     1|    0|[abysmal, service...|[abysmal, service...|[-0.0147668121836...|\n",
            "|acceptable averag...|     2|    0|[acceptable, aver...|[acceptable, aver...|[0.07128931803745...|\n",
            "|ace grunge lives ...|     1|    0|[ace, grunge, liv...|[ace, grunge, liv...|[-0.0193851339415...|\n",
            "|actually love w t...|     1|    0|[actually, love, ...|[actually, love, ...|[0.01073647486981...|\n",
            "|adequate quirky h...|     3|    0|[adequate, quirky...|[adequate, quirky...|[0.02891725992138...|\n",
            "|ah so-so sheraton...|     2|    0|[ah, so-so, shera...|[ah, so-so, shera...|[0.02877325956633...|\n",
            "|airport hotel blu...|     1|    0|[airport, hotel, ...|[airport, hotel, ...|[-0.0041358876121...|\n",
            "|all-hype, disappo...|     2|    0|[all-hype,, disap...|[all-hype,, disap...|[0.00372830754470...|\n",
            "|all-inclusive kin...|     3|    0|[all-inclusive, k...|[all-inclusive, k...|[0.00335192703340...|\n",
            "|alright casual tr...|     2|    0|[alright, casual,...|[alright, casual,...|[-0.0243072973514...|\n",
            "|amenitites listed...|     1|    0|[amenitites, list...|[amenitites, list...|[0.00246856044977...|\n",
            "|american stay awa...|     2|    0|[american, stay, ...|[american, stay, ...|[0.01985673129092...|\n",
            "|appalling disrega...|     1|    0|[appalling, disre...|[appalling, disre...|[-0.0091608765894...|\n",
            "+--------------------+------+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite on utilise une validation croisée pour optimiser les hyperparamètres du classificateur d'arbres de décision."
      ],
      "metadata": {
        "id": "XOIQENc62eDs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QD4mxPYEeKC7"
      },
      "outputs": [],
      "source": [
        "crossvaldt = CrossValidator(estimator=dt,\n",
        "                          estimatorParamMaps=dtparamGrid,\n",
        "                          evaluator=BinaryClassificationEvaluator(),\n",
        "                          numFolds=10)\n",
        "cvModeldt = crossvaldt.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI9Ivru-eKC8",
        "outputId": "3a458890-19e0-4fa2-8338-51983a635715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8220300418374996"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "tested = cvModeldt.transform(test)\n",
        "evaluation = BinaryClassificationEvaluator() #AUC\n",
        "evaluation.evaluate(tested)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "temps d'execution -->  19min"
      ],
      "metadata": {
        "id": "R1kA-xrA57_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utilisation de la régression logistique**"
      ],
      "metadata": {
        "id": "eCWMmn_8hsZg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tc80SWrMeKC8"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "paramGridlr = ParamGridBuilder() \\\n",
        "    .addGrid(lr.elasticNetParam, [0, 1]) \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .build()\n",
        "\n",
        "crossvallr = CrossValidator(estimator=lr,\n",
        "                          estimatorParamMaps=paramGridlr,\n",
        "                          evaluator=BinaryClassificationEvaluator(),\n",
        "                          numFolds=10)\n",
        "cvModellr = crossvallr.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7 min"
      ],
      "metadata": {
        "id": "nvklgg5o7_Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenez les meilleurs paramètres\n",
        "bestParams = cvModellr.bestModel.extractParamMap()\n",
        "\n",
        "# Affichez les meilleurs paramètres\n",
        "for param, value in bestParams.items():\n",
        "    print(f\"{param.name}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGdriZlIZUbS",
        "outputId": "df391f4e-29b3-4b9a-80e2-837866a58788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aggregationDepth: 2\n",
            "elasticNetParam: 0.0\n",
            "family: auto\n",
            "featuresCol: features\n",
            "fitIntercept: True\n",
            "labelCol: label\n",
            "maxBlockSizeInMB: 0.0\n",
            "maxIter: 100\n",
            "predictionCol: prediction\n",
            "probabilityCol: probability\n",
            "rawPredictionCol: rawPrediction\n",
            "regParam: 0.01\n",
            "standardization: True\n",
            "threshold: 0.5\n",
            "tol: 1e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = cvModellr.bestModel\n",
        "\n",
        "# Sauvegarder le meilleur modèle\n",
        "best_model.save(\"/content/drive/MyDrive/best_model_lr\")"
      ],
      "metadata": {
        "id": "RBz7Nq5nK5_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3J7o9V7eKC8",
        "outputId": "d0a8ae23-8e85-44e7-f697-428ab5735968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9249254881975545"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "tested = cvModellr.transform(test)\n",
        "evaluation = BinaryClassificationEvaluator() #AUC\n",
        "evaluation.evaluate(tested)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utilisation de la méthode random forest**"
      ],
      "metadata": {
        "id": "XnM_nu1shy_l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "42v4kURGeKC8"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "rfparamGrid = (ParamGridBuilder()\n",
        "               .addGrid(rf.maxDepth, [2, 5, 10])\n",
        "               .addGrid(rf.maxBins, [5, 10, 20])\n",
        "               .addGrid(rf.numTrees, [5, 20, 50])\n",
        "             .build())\n",
        "\n",
        "rfcv = CrossValidator(estimator = rf,\n",
        "                      estimatorParamMaps = rfparamGrid,\n",
        "                      evaluator = BinaryClassificationEvaluator(),\n",
        "                      numFolds = 10)\n",
        "cvModelrf = rfcv.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "38min"
      ],
      "metadata": {
        "id": "ce6yNsuTJGRp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rLFPPSPIeKC8"
      },
      "outputs": [],
      "source": [
        "best = cvModelrf.bestModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKBrsbDUeKC9"
      },
      "outputs": [],
      "source": [
        "best.save(\"/content/drive/MyDrive/model_RF\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHhTJ_eEeKC9",
        "outputId": "de565152-a51f-4408-e659-ef2f5b41e9c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "best._java_obj.getMaxDepth()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqzlk7PZeKC9",
        "outputId": "1a172291-e2ab-46ef-fbc3-b52f07b030ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "best._java_obj.getMaxBins()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07ZJWMRZeKC9",
        "outputId": "8821b751-1284-4caf-e334-d269fbca29ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "best._java_obj.getNumTrees()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCknkMsEeKC9",
        "outputId": "f295b280-0939-46b1-a5ca-1a7bc540289e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9049519603107046"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "tested = cvModelrf.transform(test)\n",
        "evaluation = BinaryClassificationEvaluator() #AUC\n",
        "evaluation.evaluate(tested)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utilisation de GBT classifier**\n",
        "(Gradient Boosted Trees)"
      ],
      "metadata": {
        "id": "XK74Nl98h8Jc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "R4zR4EcleKC-"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "gbt = GBTClassifier()\n",
        "\n",
        "gbtparamGrid = ParamGridBuilder().build()\n",
        "\n",
        "gbtcv = CrossValidator(estimator = gbt,\n",
        "                      estimatorParamMaps = gbtparamGrid,\n",
        "                      evaluator = BinaryClassificationEvaluator(),\n",
        "                      numFolds = 10)\n",
        "cvModelgbt = gbtcv.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7min"
      ],
      "metadata": {
        "id": "sDXXLXXH_5pU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZLuKcT2eKC-",
        "outputId": "d3ae5f72-9e49-4700-9ad0-d83ab54b3bc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.90122519050425"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "tested = cvModelgbt.transform(test)\n",
        "evaluation = BinaryClassificationEvaluator() #AUC\n",
        "evaluation.evaluate(tested)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utilisation du linearSVC**"
      ],
      "metadata": {
        "id": "9_0r2vSniDLA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9tYMzgXeKC-"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "\n",
        "svc = LinearSVC()\n",
        "\n",
        "svcparamGrid = (ParamGridBuilder()\n",
        "                .addGrid(svc.maxIter, [10, 100])\n",
        "                .addGrid(svc.regParam, [0.001, 0.01, 1.0,10.0])\n",
        "                .build())\n",
        "\n",
        "svccv = CrossValidator(estimator = svc,\n",
        "                      estimatorParamMaps = svcparamGrid,\n",
        "                      evaluator = BinaryClassificationEvaluator(),\n",
        "                      numFolds = 10)\n",
        "cvModelsvc = svccv.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14min"
      ],
      "metadata": {
        "id": "jG4N-7g2Mqht"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OexinCvUeKC-",
        "outputId": "beba2669-2af4-4e88-832b-16650a9861f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.001"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "bestmodel = cvModelsvc.bestModel\n",
        "bestmodel._java_obj.getRegParam()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l80gFAd5eKC-",
        "outputId": "5122e30a-1ad3-4f1e-e92d-b2e116925cc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.925238546888614"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "tested = cvModelsvc.transform(test)\n",
        "evaluation = BinaryClassificationEvaluator() #AUC\n",
        "evaluation.evaluate(tested)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2- Utilisation de la méthode hashingTF**"
      ],
      "metadata": {
        "id": "0BJKWT7kjfXS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IiRscuoleKC_"
      },
      "outputs": [],
      "source": [
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=92769)\n",
        "featurizedData = hashingTF.transform(Nstopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c7RG3NE3eKC_"
      },
      "outputs": [],
      "source": [
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features2\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "tf_idf_data = idfModel.transform(featurizedData)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataframes between 0s and 1s\n",
        "zeros_data = tf_idf_data.filter(tf_idf_data[\"Sentiment\"]==0)\n",
        "ones_data = tf_idf_data.filter(tf_idf_data[\"Sentiment\"]==1)\n",
        "# split datasets into training and testing\n",
        "train_0, test_0 = zeros_data.randomSplit([0.8,0.2])\n",
        "train_1, test_1 = ones_data.randomSplit([0.8,0.2])\n",
        "# merge datasets back together\n",
        "train_tf = train_0.union(train_1)\n",
        "test_tf = test_0.union(test_1)"
      ],
      "metadata": {
        "id": "ppqRqPNXNTOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utilisation de la regression logistique**"
      ],
      "metadata": {
        "id": "5JgCATDwmd83"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fC6NcfZgeKDA"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xb8y0NBneKDA"
      },
      "outputs": [],
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.elasticNetParam, [0, 1]) \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .build()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tf = train.withColumnRenamed(\"Sentiment\", \"label\")\n",
        "test_tf = test.withColumnRenamed(\"Sentiment\", \"label\")"
      ],
      "metadata": {
        "id": "Hg6e25WdN-ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EjznfQJDeKDA"
      },
      "outputs": [],
      "source": [
        "crossval = CrossValidator(estimator=lr,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=BinaryClassificationEvaluator(),\n",
        "                          numFolds=10)\n",
        "cvModel = crossval.fit(train_tf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8min"
      ],
      "metadata": {
        "id": "aH9qd5ThnVX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tested = cvModel.transform(test_tf)\n",
        "evaluation = BinaryClassificationEvaluator() #AUC\n",
        "evaluation.evaluate(tested)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPyYQpDqQCCg",
        "outputId": "f03eb5fd-8069-48db-f65c-dcd873a0caa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.924921359302901"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassificationModel\n",
        "\n",
        "# Charger le modèle RandomForestClassificationModel\n",
        "loaded_rf_model = RandomForestClassificationModel.load(\"/content/drive/MyDrive/model\")\n",
        "\n",
        "predictions = loaded_rf_model.transform(test)"
      ],
      "metadata": {
        "id": "2sglBq3jHdXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = BinaryClassificationEvaluator() #AUC\n",
        "evaluation.evaluate(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOsuUZE0J1EX",
        "outputId": "7c0d800c-73be-4deb-8e50-56de0735e962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6327256884495065"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegressionModel\n",
        "\n",
        "# Charger le modèle RandomForestClassificationModel\n",
        "loaded_rf_model = LogisticRegressionModel.load(\"/content/drive/MyDrive/best_model_lr\")\n",
        "\n",
        "predictions = loaded_rf_model.transform(test)"
      ],
      "metadata": {
        "id": "gd7V722QL62S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = BinaryClassificationEvaluator() #AUC\n",
        "evaluation.evaluate(predictions)"
      ],
      "metadata": {
        "id": "TY76-jvHMMwB",
        "outputId": "caf39db4-e0e2-4876-9d56-54880852e8f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9208090209256902"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}